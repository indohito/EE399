{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43577077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for rho 10\n",
      "Epoch 0 Loss: 43.4697\n",
      "Epoch 0 Loss: 42.3551\n",
      "Epoch 0 Loss: 43.6862\n",
      "Epoch 0 Loss: 35.8543\n",
      "Epoch 10 Loss: 43.5300\n",
      "Epoch 10 Loss: 41.1086\n",
      "Epoch 10 Loss: 43.6283\n",
      "Epoch 10 Loss: 45.8069\n",
      "Epoch 20 Loss: 42.9457\n",
      "Epoch 20 Loss: 41.4997\n",
      "Epoch 20 Loss: 44.1599\n",
      "Epoch 20 Loss: 43.0997\n",
      "Epoch 30 Loss: 42.0032\n",
      "Epoch 30 Loss: 44.8053\n",
      "Epoch 30 Loss: 41.3476\n",
      "Epoch 30 Loss: 46.6941\n",
      "Epoch 40 Loss: 41.4990\n",
      "Epoch 40 Loss: 42.5719\n",
      "Epoch 40 Loss: 43.0363\n",
      "Epoch 40 Loss: 55.0838\n",
      "Epoch 50 Loss: 45.5434\n",
      "Epoch 50 Loss: 42.8108\n",
      "Epoch 50 Loss: 40.2377\n",
      "Epoch 50 Loss: 43.2071\n",
      "Epoch 60 Loss: 42.6618\n",
      "Epoch 60 Loss: 41.7803\n",
      "Epoch 60 Loss: 43.5765\n",
      "Epoch 60 Loss: 47.7935\n",
      "Epoch 70 Loss: 40.1449\n",
      "Epoch 70 Loss: 43.9480\n",
      "Epoch 70 Loss: 43.6274\n",
      "Epoch 70 Loss: 50.1799\n",
      "Epoch 80 Loss: 42.6227\n",
      "Epoch 80 Loss: 44.1210\n",
      "Epoch 80 Loss: 41.1488\n",
      "Epoch 80 Loss: 48.8016\n",
      "Epoch 90 Loss: 44.0885\n",
      "Epoch 90 Loss: 43.0969\n",
      "Epoch 90 Loss: 42.0763\n",
      "Epoch 90 Loss: 37.8485\n",
      " \n",
      " \n",
      "Train for rho 28\n",
      "Epoch 0 Loss: 314.2322\n",
      "Epoch 0 Loss: 309.3161\n",
      "Epoch 0 Loss: 304.7562\n",
      "Epoch 0 Loss: 307.3682\n",
      "Epoch 10 Loss: 304.8739\n",
      "Epoch 10 Loss: 314.8662\n",
      "Epoch 10 Loss: 309.5475\n",
      "Epoch 10 Loss: 299.5031\n",
      "Epoch 20 Loss: 307.1765\n",
      "Epoch 20 Loss: 315.4046\n",
      "Epoch 20 Loss: 306.0738\n",
      "Epoch 20 Loss: 304.5656\n",
      "Epoch 30 Loss: 313.3616\n",
      "Epoch 30 Loss: 302.7162\n",
      "Epoch 30 Loss: 311.5654\n",
      "Epoch 30 Loss: 312.6582\n",
      "Epoch 40 Loss: 306.1543\n",
      "Epoch 40 Loss: 310.3315\n",
      "Epoch 40 Loss: 310.8557\n",
      "Epoch 40 Loss: 315.0722\n",
      "Epoch 50 Loss: 312.4330\n",
      "Epoch 50 Loss: 310.9268\n",
      "Epoch 50 Loss: 306.7606\n",
      "Epoch 50 Loss: 292.8403\n",
      "Epoch 60 Loss: 307.9545\n",
      "Epoch 60 Loss: 308.4760\n",
      "Epoch 60 Loss: 310.1649\n",
      "Epoch 60 Loss: 321.0406\n",
      "Epoch 70 Loss: 313.9046\n",
      "Epoch 70 Loss: 303.6160\n",
      "Epoch 70 Loss: 309.5198\n",
      "Epoch 70 Loss: 317.4812\n",
      "Epoch 80 Loss: 311.0876\n",
      "Epoch 80 Loss: 309.0786\n",
      "Epoch 80 Loss: 306.2597\n",
      "Epoch 80 Loss: 322.3963\n",
      "Epoch 90 Loss: 310.0615\n",
      "Epoch 90 Loss: 306.9225\n",
      "Epoch 90 Loss: 309.7044\n",
      "Epoch 90 Loss: 320.2971\n",
      " \n",
      " \n",
      "Train for rho 40\n",
      "Epoch 0 Loss: 836.0850\n",
      "Epoch 0 Loss: 832.5369\n",
      "Epoch 0 Loss: 835.0889\n",
      "Epoch 0 Loss: 836.8813\n",
      "Epoch 10 Loss: 836.7608\n",
      "Epoch 10 Loss: 835.8785\n",
      "Epoch 10 Loss: 831.7216\n",
      "Epoch 10 Loss: 831.6805\n",
      "Epoch 20 Loss: 836.3810\n",
      "Epoch 20 Loss: 832.9444\n",
      "Epoch 20 Loss: 832.7064\n",
      "Epoch 20 Loss: 850.3140\n",
      "Epoch 30 Loss: 835.4990\n",
      "Epoch 30 Loss: 837.7640\n",
      "Epoch 30 Loss: 830.9921\n",
      "Epoch 30 Loss: 832.5266\n",
      "Epoch 40 Loss: 828.8383\n",
      "Epoch 40 Loss: 839.7453\n",
      "Epoch 40 Loss: 835.1224\n",
      "Epoch 40 Loss: 836.9200\n",
      "Epoch 50 Loss: 832.0699\n",
      "Epoch 50 Loss: 836.6442\n",
      "Epoch 50 Loss: 835.1472\n",
      "Epoch 50 Loss: 835.6781\n",
      "Epoch 60 Loss: 834.0881\n",
      "Epoch 60 Loss: 835.4624\n",
      "Epoch 60 Loss: 835.6064\n",
      "Epoch 60 Loss: 825.3123\n",
      "Epoch 70 Loss: 838.4496\n",
      "Epoch 70 Loss: 830.7043\n",
      "Epoch 70 Loss: 835.9184\n",
      "Epoch 70 Loss: 825.9903\n",
      "Epoch 80 Loss: 832.7200\n",
      "Epoch 80 Loss: 836.2329\n",
      "Epoch 80 Loss: 835.8349\n",
      "Epoch 80 Loss: 828.2660\n",
      "Epoch 90 Loss: 834.6874\n",
      "Epoch 90 Loss: 835.0126\n",
      "Epoch 90 Loss: 836.5355\n",
      "Epoch 90 Loss: 816.6838\n",
      " \n",
      " \n",
      "Test_Loss for 17: 165.0996\n",
      "Test_Loss for 35: 637.2499\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/indohito/EE399/tree/main/EE399HW5\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "\n",
    "# Define the neural network architecture\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=128, output_size=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(FFNN().parameters(), lr=0.01)\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, dataloader, criterion, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loss_record = []\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_record.append(loss.item())\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n",
    "\n",
    "dt = 0.01\n",
    "T = 8\n",
    "t = np.arange(0,T+dt,dt)\n",
    "beta = 8/3\n",
    "sigma = 10\n",
    "rho =10\n",
    "def lorenz_deriv(x_y_z, t0, sigma=sigma, beta=beta, rho=rho):\n",
    "    x, y, z = x_y_z\n",
    "    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
    "        \n",
    "\n",
    "\n",
    "for rho in [10, 28, 40]:\n",
    "    # Generate the training data\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho))\n",
    "                      for x0_j in x0])\n",
    "    nn_input = torch.tensor(x_t[:,:-1,:], dtype=torch.float32)\n",
    "    nn_output = torch.tensor(x_t[:,1:,:], dtype=torch.float32)\n",
    "    dataset = TensorDataset(nn_input, nn_output)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train the model\n",
    "    model = FFNN()\n",
    "    print(f\"Train for rho {rho}\")\n",
    "    train(model, dataloader, criterion, optimizer)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "# Use the trained model for future state prediction for ρ = 17 and ρ = 35\n",
    "for rho in [17, 35]:\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho))\n",
    "                      for x0_j in x0])\n",
    "    nn_input = torch.tensor(x_t[:,:-1,:], dtype=torch.float32)\n",
    "    nn_output = torch.tensor(x_t[:,1:,:], dtype=torch.float32)\n",
    "    dataset = TensorDataset(nn_input, nn_output)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            test_loss = criterion(outputs, targets)\n",
    "        print(f'Test_Loss for {rho}: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cd8025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for rho 10\n",
      "Epoch 0 Loss: 44.4304\n",
      "Epoch 0 Loss: 44.6278\n",
      "Epoch 0 Loss: 45.5824\n",
      "Epoch 0 Loss: 45.4888\n",
      "Epoch 10 Loss: 45.1564\n",
      "Epoch 10 Loss: 44.8943\n",
      "Epoch 10 Loss: 44.4646\n",
      "Epoch 10 Loss: 46.3868\n",
      "Epoch 20 Loss: 45.4672\n",
      "Epoch 20 Loss: 44.3075\n",
      "Epoch 20 Loss: 44.7701\n",
      "Epoch 20 Loss: 46.0241\n",
      "Epoch 30 Loss: 44.7416\n",
      "Epoch 30 Loss: 45.4595\n",
      "Epoch 30 Loss: 44.4589\n",
      "Epoch 30 Loss: 45.3312\n",
      "Epoch 40 Loss: 44.6869\n",
      "Epoch 40 Loss: 44.6073\n",
      "Epoch 40 Loss: 45.2477\n",
      "Epoch 40 Loss: 46.1613\n",
      "Epoch 50 Loss: 44.8667\n",
      "Epoch 50 Loss: 44.8879\n",
      "Epoch 50 Loss: 44.6814\n",
      "Epoch 50 Loss: 47.0288\n",
      "Epoch 60 Loss: 44.6303\n",
      "Epoch 60 Loss: 45.1631\n",
      "Epoch 60 Loss: 45.1351\n",
      "Epoch 60 Loss: 43.1314\n",
      "Epoch 70 Loss: 44.9580\n",
      "Epoch 70 Loss: 44.3382\n",
      "Epoch 70 Loss: 45.7687\n",
      "Epoch 70 Loss: 42.0456\n",
      "Epoch 80 Loss: 45.3974\n",
      "Epoch 80 Loss: 44.7592\n",
      "Epoch 80 Loss: 44.4127\n",
      "Epoch 80 Loss: 45.9768\n",
      "Epoch 90 Loss: 45.4632\n",
      "Epoch 90 Loss: 44.1198\n",
      "Epoch 90 Loss: 45.1004\n",
      "Epoch 90 Loss: 44.9254\n",
      " \n",
      " \n",
      "Train for rho 28\n",
      "Epoch 0 Loss: 279.4544\n",
      "Epoch 0 Loss: 280.2749\n",
      "Epoch 0 Loss: 277.5621\n",
      "Epoch 0 Loss: 275.2833\n",
      "Epoch 10 Loss: 278.6010\n",
      "Epoch 10 Loss: 277.7872\n",
      "Epoch 10 Loss: 280.4754\n",
      "Epoch 10 Loss: 278.9192\n",
      "Epoch 20 Loss: 277.6618\n",
      "Epoch 20 Loss: 279.4121\n",
      "Epoch 20 Loss: 278.9236\n",
      "Epoch 20 Loss: 285.6909\n",
      "Epoch 30 Loss: 282.4055\n",
      "Epoch 30 Loss: 280.2297\n",
      "Epoch 30 Loss: 275.5367\n",
      "Epoch 30 Loss: 268.1169\n",
      "Epoch 40 Loss: 280.3430\n",
      "Epoch 40 Loss: 279.5352\n",
      "Epoch 40 Loss: 277.6007\n",
      "Epoch 40 Loss: 273.9682\n",
      "Epoch 50 Loss: 280.5527\n",
      "Epoch 50 Loss: 280.9235\n",
      "Epoch 50 Loss: 275.8663\n",
      "Epoch 50 Loss: 274.9872\n",
      "Epoch 60 Loss: 279.8145\n",
      "Epoch 60 Loss: 276.7957\n",
      "Epoch 60 Loss: 278.9865\n",
      "Epoch 60 Loss: 288.7918\n",
      "Epoch 70 Loss: 280.6094\n",
      "Epoch 70 Loss: 276.6819\n",
      "Epoch 70 Loss: 280.9204\n",
      "Epoch 70 Loss: 268.1114\n",
      "Epoch 80 Loss: 276.6365\n",
      "Epoch 80 Loss: 280.7080\n",
      "Epoch 80 Loss: 278.0481\n",
      "Epoch 80 Loss: 290.4970\n",
      "Epoch 90 Loss: 274.9861\n",
      "Epoch 90 Loss: 278.9337\n",
      "Epoch 90 Loss: 281.6464\n",
      "Epoch 90 Loss: 288.9643\n",
      " \n",
      " \n",
      "Train for rho 40\n",
      "Epoch 0 Loss: 553.1006\n",
      "Epoch 0 Loss: 552.0800\n",
      "Epoch 0 Loss: 548.6508\n",
      "Epoch 0 Loss: 553.3997\n",
      "Epoch 10 Loss: 552.0618\n",
      "Epoch 10 Loss: 550.3674\n",
      "Epoch 10 Loss: 551.5827\n",
      "Epoch 10 Loss: 551.9249\n",
      "Epoch 20 Loss: 552.2319\n",
      "Epoch 20 Loss: 551.2910\n",
      "Epoch 20 Loss: 550.1160\n",
      "Epoch 20 Loss: 554.9330\n",
      "Epoch 30 Loss: 551.3167\n",
      "Epoch 30 Loss: 552.1209\n",
      "Epoch 30 Loss: 550.6459\n",
      "Epoch 30 Loss: 551.3567\n",
      "Epoch 40 Loss: 552.0189\n",
      "Epoch 40 Loss: 550.6564\n",
      "Epoch 40 Loss: 551.0789\n",
      "Epoch 40 Loss: 554.0046\n",
      "Epoch 50 Loss: 551.4738\n",
      "Epoch 50 Loss: 550.1005\n",
      "Epoch 50 Loss: 552.3854\n",
      "Epoch 50 Loss: 552.3792\n",
      "Epoch 60 Loss: 552.5836\n",
      "Epoch 60 Loss: 550.5898\n",
      "Epoch 60 Loss: 551.0229\n",
      "Epoch 60 Loss: 550.4102\n",
      "Epoch 70 Loss: 550.2289\n",
      "Epoch 70 Loss: 552.5055\n",
      "Epoch 70 Loss: 550.8777\n",
      "Epoch 70 Loss: 555.1356\n",
      "Epoch 80 Loss: 550.6511\n",
      "Epoch 80 Loss: 550.5611\n",
      "Epoch 80 Loss: 552.1895\n",
      "Epoch 80 Loss: 556.7649\n",
      "Epoch 90 Loss: 550.9655\n",
      "Epoch 90 Loss: 550.3206\n",
      "Epoch 90 Loss: 552.6741\n",
      "Epoch 90 Loss: 552.3351\n",
      " \n",
      " \n",
      "Test_Loss for 17: 109.0322\n",
      "Test_Loss for 35: 432.1075\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=3, hidden_size=50, num_layers=3)\n",
    "        self.fc1 = nn.Linear(in_features=50, out_features=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_lstm = optim.Adam(LSTM().parameters(), lr=0.0001)\n",
    "\n",
    "for rho in [10, 28, 40]:\n",
    "    # Generate the training data\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho))\n",
    "                      for x0_j in x0])\n",
    "    nn_input = torch.tensor(x_t[:,:-1,:], dtype=torch.float32)\n",
    "    nn_output = torch.tensor(x_t[:,1:,:], dtype=torch.float32)\n",
    "    dataset = TensorDataset(nn_input, nn_output)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train the model\n",
    "    model = LSTM()\n",
    "    print(f\"Train for rho {rho}\")\n",
    "    train(model, dataloader, criterion, optimizer_lstm)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "# Use the trained model for future state prediction for ρ = 17 and ρ = 35\n",
    "for rho in [17, 35]:\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho))\n",
    "                      for x0_j in x0])\n",
    "    nn_input = torch.tensor(x_t[:,:-1,:], dtype=torch.float32)\n",
    "    nn_output = torch.tensor(x_t[:,1:,:], dtype=torch.float32)\n",
    "    dataset = TensorDataset(nn_input, nn_output)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            test_loss = criterion(outputs, targets)\n",
    "        print(f'Test_Loss for {rho}: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f1e7444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for rho 10\n",
      "Epoch 0 Loss: 44.1994\n",
      "Epoch 0 Loss: 44.5685\n",
      "Epoch 0 Loss: 44.8176\n",
      "Epoch 0 Loss: 43.0061\n",
      "Epoch 10 Loss: 44.6095\n",
      "Epoch 10 Loss: 44.3365\n",
      "Epoch 10 Loss: 44.6395\n",
      "Epoch 10 Loss: 42.8961\n",
      "Epoch 20 Loss: 44.2318\n",
      "Epoch 20 Loss: 44.6378\n",
      "Epoch 20 Loss: 44.2247\n",
      "Epoch 20 Loss: 47.0013\n",
      "Epoch 30 Loss: 44.7016\n",
      "Epoch 30 Loss: 43.5682\n",
      "Epoch 30 Loss: 45.3113\n",
      "Epoch 30 Loss: 43.4281\n",
      "Epoch 40 Loss: 44.2555\n",
      "Epoch 40 Loss: 45.1700\n",
      "Epoch 40 Loss: 44.0624\n",
      "Epoch 40 Loss: 43.6813\n",
      "Epoch 50 Loss: 44.0337\n",
      "Epoch 50 Loss: 44.5998\n",
      "Epoch 50 Loss: 44.7982\n",
      "Epoch 50 Loss: 44.1622\n",
      "Epoch 60 Loss: 44.4345\n",
      "Epoch 60 Loss: 44.3034\n",
      "Epoch 60 Loss: 44.4775\n",
      "Epoch 60 Loss: 45.7818\n",
      "Epoch 70 Loss: 44.3964\n",
      "Epoch 70 Loss: 44.2081\n",
      "Epoch 70 Loss: 45.0556\n",
      "Epoch 70 Loss: 42.9063\n",
      "Epoch 80 Loss: 44.9155\n",
      "Epoch 80 Loss: 44.5275\n",
      "Epoch 80 Loss: 44.0376\n",
      "Epoch 80 Loss: 43.5440\n",
      "Epoch 90 Loss: 44.2817\n",
      "Epoch 90 Loss: 44.3503\n",
      "Epoch 90 Loss: 44.8280\n",
      "Epoch 90 Loss: 44.0628\n",
      " \n",
      " \n",
      "Train for rho 28\n",
      "Epoch 0 Loss: 278.0729\n",
      "Epoch 0 Loss: 278.0535\n",
      "Epoch 0 Loss: 279.8804\n",
      "Epoch 0 Loss: 279.0417\n",
      "Epoch 10 Loss: 277.3924\n",
      "Epoch 10 Loss: 278.7848\n",
      "Epoch 10 Loss: 280.1633\n",
      "Epoch 10 Loss: 276.7020\n",
      "Epoch 20 Loss: 280.5601\n",
      "Epoch 20 Loss: 280.1714\n",
      "Epoch 20 Loss: 275.1096\n",
      "Epoch 20 Loss: 279.9429\n",
      "Epoch 30 Loss: 276.0471\n",
      "Epoch 30 Loss: 280.4295\n",
      "Epoch 30 Loss: 279.8627\n",
      "Epoch 30 Loss: 276.0682\n",
      "Epoch 40 Loss: 278.2453\n",
      "Epoch 40 Loss: 278.0363\n",
      "Epoch 40 Loss: 280.6075\n",
      "Epoch 40 Loss: 272.9988\n",
      "Epoch 50 Loss: 280.7989\n",
      "Epoch 50 Loss: 277.3252\n",
      "Epoch 50 Loss: 278.3860\n",
      "Epoch 50 Loss: 275.4773\n",
      "Epoch 60 Loss: 278.3701\n",
      "Epoch 60 Loss: 279.2314\n",
      "Epoch 60 Loss: 278.4961\n",
      "Epoch 60 Loss: 278.7833\n",
      "Epoch 70 Loss: 278.2887\n",
      "Epoch 70 Loss: 279.9300\n",
      "Epoch 70 Loss: 278.5487\n",
      "Epoch 70 Loss: 272.9559\n",
      "Epoch 80 Loss: 282.3638\n",
      "Epoch 80 Loss: 279.0505\n",
      "Epoch 80 Loss: 274.2346\n",
      "Epoch 80 Loss: 282.1919\n",
      "Epoch 90 Loss: 280.5891\n",
      "Epoch 90 Loss: 277.3654\n",
      "Epoch 90 Loss: 277.5438\n",
      "Epoch 90 Loss: 284.3773\n",
      " \n",
      " \n",
      "Train for rho 40\n",
      "Epoch 0 Loss: 562.2065\n",
      "Epoch 0 Loss: 562.0587\n",
      "Epoch 0 Loss: 560.2834\n",
      "Epoch 0 Loss: 559.6774\n",
      "Epoch 10 Loss: 560.3966\n",
      "Epoch 10 Loss: 562.4970\n",
      "Epoch 10 Loss: 561.4911\n",
      "Epoch 10 Loss: 561.9114\n",
      "Epoch 20 Loss: 561.1606\n",
      "Epoch 20 Loss: 562.0964\n",
      "Epoch 20 Loss: 561.2552\n",
      "Epoch 20 Loss: 561.0954\n",
      "Epoch 30 Loss: 561.7744\n",
      "Epoch 30 Loss: 560.6418\n",
      "Epoch 30 Loss: 562.1550\n",
      "Epoch 30 Loss: 560.5637\n",
      "Epoch 40 Loss: 561.4102\n",
      "Epoch 40 Loss: 560.2711\n",
      "Epoch 40 Loss: 562.4467\n",
      "Epoch 40 Loss: 564.2808\n",
      "Epoch 50 Loss: 562.1592\n",
      "Epoch 50 Loss: 560.9583\n",
      "Epoch 50 Loss: 561.4972\n",
      "Epoch 50 Loss: 559.5932\n",
      "Epoch 60 Loss: 561.0815\n",
      "Epoch 60 Loss: 564.1064\n",
      "Epoch 60 Loss: 559.4836\n",
      "Epoch 60 Loss: 559.8457\n",
      "Epoch 70 Loss: 560.6337\n",
      "Epoch 70 Loss: 560.5386\n",
      "Epoch 70 Loss: 562.5279\n",
      "Epoch 70 Loss: 567.2393\n",
      "Epoch 80 Loss: 560.3117\n",
      "Epoch 80 Loss: 560.8915\n",
      "Epoch 80 Loss: 563.1895\n",
      "Epoch 80 Loss: 562.0035\n",
      "Epoch 90 Loss: 561.6271\n",
      "Epoch 90 Loss: 563.9988\n",
      "Epoch 90 Loss: 559.1050\n",
      "Epoch 90 Loss: 558.7294\n",
      " \n",
      " \n",
      "Test_Loss for 17: 120.1754\n",
      "Test_Loss for 35: 433.0215\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=3, hidden_size=60, num_layers=3)\n",
    "        self.fc1 = nn.Linear(in_features=60, out_features=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "criterion = nn.MSELoss()\n",
    "optimizer_rnn = optim.Adam(RNN().parameters(), lr=0.01)\n",
    "\n",
    "for rho in [10, 28, 40]:\n",
    "    # Generate the training data\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho))\n",
    "                      for x0_j in x0])\n",
    "    nn_input = torch.tensor(x_t[:,:-1,:], dtype=torch.float32)\n",
    "    nn_output = torch.tensor(x_t[:,1:,:], dtype=torch.float32)\n",
    "    dataset = TensorDataset(nn_input, nn_output)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train the model\n",
    "    model = RNN()\n",
    "    print(f\"Train for rho {rho}\")\n",
    "    train(model, dataloader, criterion, optimizer_rnn)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "# Use the trained model for future state prediction for ρ = 17 and ρ = 35\n",
    "for rho in [17, 35]:\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho))\n",
    "                      for x0_j in x0])\n",
    "    nn_input = torch.tensor(x_t[:,:-1,:], dtype=torch.float32)\n",
    "    nn_output = torch.tensor(x_t[:,1:,:], dtype=torch.float32)\n",
    "    dataset = TensorDataset(nn_input, nn_output)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            test_loss = criterion(outputs, targets)\n",
    "        print(f'Test_Loss for {rho}: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for rho 10\n",
      "Epoch 0 Loss: 45.0758\n",
      "Epoch 0 Loss: 44.5551\n",
      "Epoch 0 Loss: 44.8880\n",
      "Epoch 0 Loss: 45.7044\n",
      "Epoch 10 Loss: 44.3023\n",
      "Epoch 10 Loss: 44.9313\n",
      "Epoch 10 Loss: 45.1238\n",
      "Epoch 10 Loss: 46.9824\n",
      "Epoch 20 Loss: 44.7238\n",
      "Epoch 20 Loss: 44.7148\n",
      "Epoch 20 Loss: 45.2225\n",
      "Epoch 20 Loss: 44.5666\n",
      "Epoch 30 Loss: 44.9784\n",
      "Epoch 30 Loss: 44.7087\n",
      "Epoch 30 Loss: 44.6958\n",
      "Epoch 30 Loss: 46.7870\n",
      "Epoch 40 Loss: 45.0413\n",
      "Epoch 40 Loss: 44.5800\n",
      "Epoch 40 Loss: 45.0185\n",
      "Epoch 40 Loss: 44.7328\n",
      "Epoch 50 Loss: 45.1211\n",
      "Epoch 50 Loss: 44.5101\n",
      "Epoch 50 Loss: 44.9537\n",
      "Epoch 50 Loss: 45.1677\n",
      "Epoch 60 Loss: 45.0223\n",
      "Epoch 60 Loss: 44.2101\n",
      "Epoch 60 Loss: 44.9057\n",
      "Epoch 60 Loss: 48.7551\n",
      "Epoch 70 Loss: 45.0062\n",
      "Epoch 70 Loss: 44.7421\n",
      "Epoch 70 Loss: 44.8358\n",
      "Epoch 70 Loss: 45.1817\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Reservoir(nn.Module):\n",
    "    def __init__(self, hidden_dim, connectivity):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.Wx = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.Wh = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.Uh = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.register_buffer('Wx_mask', (torch.rand(hidden_dim, hidden_dim) < connectivity).float())\n",
    "        self.register_buffer('Wh_mask', (torch.rand(hidden_dim, hidden_dim) < connectivity).float())\n",
    "        self.register_buffer('Uh_mask', (torch.rand(hidden_dim, hidden_dim) < connectivity).float())\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        h = self.act(torch.mm(h, self.Uh*self.Uh_mask) + torch.mm(x, self.Wh*self.Wh_mask))\n",
    "        y = self.act(torch.mm(h, self.Wx*self.Wx_mask))\n",
    "        return y, h\n",
    "    \n",
    "class ESN(nn.Module):\n",
    "    def __init__(self, in_dim=3, out_dim=3, reservoir_dim=100, connectivity=.6):\n",
    "        super().__init__()\n",
    "        self.reservoir_dim = reservoir_dim\n",
    "        self.input_to_reservoir = nn.Linear(in_dim, reservoir_dim)\n",
    "        self.input_to_reservoir.requires_grad_(False)\n",
    "        self.reservoir = Reservoir(reservoir_dim, connectivity)\n",
    "        self.readout = nn.Linear(reservoir_dim, out_dim)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        reservoir_in = self.input_to_reservoir(x)\n",
    "        h = torch.ones(x.size(0), self.reservoir_dim)\n",
    "        reservoirs = []\n",
    "        for i in range(x.size(1)):\n",
    "            out, h = self.reservoir(reservoir_in[:, i, :], h)\n",
    "            reservoirs.append(out.unsqueeze(1))\n",
    "        reservoirs = torch.cat(reservoirs, dim=1)\n",
    "        outputs = self.readout(reservoirs)\n",
    "        return outputs\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_esn = optim.SGD(ESN().parameters(), lr=0.001)\n",
    "\n",
    "for rho in [10, 28, 40]:\n",
    "    # Generate the training data\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho))\n",
    "                      for x0_j in x0])\n",
    "    nn_input = torch.tensor(x_t[:,:-1,:], dtype=torch.float32)\n",
    "    nn_output = torch.tensor(x_t[:,1:,:], dtype=torch.float32)\n",
    "    dataset = TensorDataset(nn_input, nn_output)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train the model\n",
    "    model = ESN()\n",
    "    print(f\"Train for rho {rho}\")\n",
    "    train(model, dataloader, criterion, optimizer_esn)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "# Use the trained model for future state prediction for ρ = 17 and ρ = 35\n",
    "for rho in [17, 35]:\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho))\n",
    "                      for x0_j in x0])\n",
    "    nn_input = torch.tensor(x_t[:,:-1,:], dtype=torch.float32)\n",
    "    nn_output = torch.tensor(x_t[:,1:,:], dtype=torch.float32)\n",
    "    dataset = TensorDataset(nn_input, nn_output)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            test_loss = criterion(outputs, targets)\n",
    "        print(f'Test_Loss for {rho}: {test_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe8f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
